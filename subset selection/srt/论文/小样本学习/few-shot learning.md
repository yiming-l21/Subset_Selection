## few-shot learning

### 基础知识

#### 集成学习

集成学习通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统。集成学习的一般结构为先产生一组个体学习器，再用某种策略将它们结合起来。个体学习器通常由一个现有学习算法从训练数据产生，例如决策树算法、BP神经网络算法等。

![](C:\Users\liuyi\Desktop\论文\小样本学习\image\1.jpg)

同质集中中只包含同类个体学习器，如神经网络集成中全部是神经网络，这样的个体学习器称为基学习器(base learner)

集成学习通过多个学习器进行结合，可获得更好的泛化性能，这对弱学习器尤为明显。(弱学习器指泛化性能略优于随机猜测的学习器)

**个体学习器的准确性和多样性本身存在冲突，如何产生并结合好而不同的个体学习器，是集成学习研究的核心。** 

![](C:\Users\liuyi\Desktop\论文\小样本学习\image\好而不同.png)

目前集成学习方法分为两大类：个体学习器存在强依赖关系，必须串行生成的序列化方法(Boosting)；个体学习器不存在强依赖关系，同时生成的并行化方法。(Bagging,Random Forest)

#### Boosting串行生成序列化方法

![](C:\Users\liuyi\Desktop\论文\小样本学习\image\22.jpg)

**AdaBoost**

输入：训练集D={(x1,y1),(x2,y2)…(xm,ym)};基学习算法Sigma;训练轮数T

过程：

$D_1(x)=1/m$ //初始化样本的权值分布

for t=1,2…T do

​	$h_t=Sigma(D,D_t)$ //训练出分类器learner T

​	$\epsilon_t=P_{x-D_t}(h_t(x)≠f(x))$//计算误差

​	if  $\epsilon_t$>0.5 then break;

​	$\alpha_t=\frac{1}{2}ln(\frac{1-\epsilon_t}{\epsilon})$ //计算分类器ht的权重,本质是损失函数对学习率导数的极值点，s.t.损失最小化

​	$D_{t+1}(x)=\frac{D_t(x)}{Z_t}*exp(-\alpha_tf(x)h_t(x))$ //计算正确的权重变小，计算错误的权重($f(x)h_t(x)<0$ )变大

end for

$H(x)=sign(\sum_{t=1}^T\alpha_th_t(x))$ 

#### **Bagging和随机森林** 并行化方法

为了能够做到并行化，个体学习器应该尽可能独立，为了做到独立，**可以把训练样本进行采样，产生若干不同子集，从每个训练子集中得到一个基学习器** 。这样，可以保证基学习器间存在较大差异。

**Bagging**

输入：训练集D={(x1,y1),(x2,y2)…(xm,ym)};基学习算法Sigma;训练轮数T

过程：

for t=1,2...T do

​		$h_t=Sigma(D,D_{bs})$ //训练出分类器learner T,$D_{bs}$ 是自助采样产生的样本分布。

end for

输出：

$H(x)=\underset{y\in Y}{arg max}(\sum_{t=1}^TI(h_t(x)=y))$ //argmax是简单的相同权重的投票原则

###### remark:

1.基学习器的计算复杂度为O(m)的话，Bagging的时间复杂度约为O(Tm),是一个非常高效的算法。

2.自助采样过程有些样本重复出现，有的从未出现。剩下未出现的样本可以作为验证集对泛化性能进行**包外估计** ，当基学习器是决策树时，可使用包外样本进行剪枝，当基学习器是神经网络时可以用包外样本辅助早期停止以减少过拟合的风险。

**随机森林**

随机森林是以决策树为基学习器构建Bagging的基础上，进一步在决策树模型的训练过程中引入了随机属性选择。

传统决策树选择划分属性时从d个属性中选择一个最优属性。在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，再从子集中选择最优属性划分。一般情况下$k=log_2d$ 

随机森林使得基学习器的多样性不仅来自于样本扰动，还来自于属性扰动，这使diversity进一步提升。

#### 结合策略

##### 为什么集成学习是有效的？

1.防止误选模型导致泛化能力不佳

2.防止学习算法陷入局部最优

3.扩大假设空间，学得更好近似。

**平均法** 

简单平均和加权平均。集成学习中各种结合方法都是其特例或者变体，是集成学习研究的基本出发点，不同集成学习方法可以看作**用不同的方式来确定基学习器权重** 

权重一般在样本中学习得到，对于大规模集成，学习权重过多会存在过拟合，因此对于diversity较大的学习器用加权平均，性能相近时用简单平均。

**投票法** 

绝对多数投票：某标记得票超过半数，则预测为标记。若不存在绝对多数，则拒绝。

相对多数投票：预测为得票最多的标记。

**学习法**

对于大训练数据，学习法是更强大的结合策略，即通过与另一个学习器来进行结合。Stacking是学习法的代表，个体学习器成为初级学习器，用于结合的学习器成为meta-learner.

**stacking**

输入：训练集D={(x1,y1),(x2,y2)…(xm,ym)};

基学习算法Sigma1，Sigma2...SigmaT;

元学习器Sigma

过程：

for t=1,2..T do //使用初级学习算法产生初级学习器ht

​	$h_t=Sigma_t(D)$ 

end for

$D'=\phi$ 

for i=1,2...m do

​	for t =1,2..T do

​		 $z_{it}=h_t(x_i)$ 

​	end for

​	$D'=D'+((z_{i1},z_{i2}...z_{iT}),y_i)$ //用$h_1(x_i),..h_t(x_i)$ 预测yi

end for

h'=Sigma(D')

输出：$H(x)=h'(h1(x),h2(x)..hT(x))$ 

#### 多样性

##### 多样性度量

![](C:\Users\liuyi\Desktop\论文\小样本学习\image\4.jpg)

##### 多样性增强

**数据样本扰动**

Bagging中自助采样，AdaBoost中的序列采样等

**输入属性扰动**

![](C:\Users\liuyi\Desktop\论文\小样本学习\image\5.jpg)

**输出表示扰动**

翻转法，输出调制法

**算法参数扰动**

负相关法



### 论文研读

#### introduction

Few-shot classification旨在训练一个分类器识别未见过的类，每个类中只有少数标记样本（shots），这在学术和实践上都具有重要意义。

集成少样本分类可以结合任何少样本分类器（基学习器）以获得更好的性能，所以它是使用最广泛的最新方法。

先前的研究表明，集成模型的性能在很大程度上受到个体基础学习器之间的多样性和合作以及镜头的代表性的影响。因此，使用所有学习器和镜头可能会降低性能。例如，如果一个学习者的表现很差，它的预测与大多数人不同，就会损害集成模型的表现。此外，错误地表示某些样本的镜头通常会导致这些样本的错误分类。因此，需要选择一个合作和多样化学习者的子集，并确定一小组具有代表性的镜头，这对于少镜头分类的实际应用来说是一个长期存在的挑战。

现有的学习方法通常将集成模型应用于所有给定的学习器和镜头，这往往无法实现最佳性能。提高性能通常需要反复选择学习者并调整他们的权重。如果不全面了解模型和镜头如何协同工作以达到最终预测，这种试错过程非常耗时且需要专业知识。此外，由于缺乏镜头的精细化，性能提升有限。

为了有效地提高性能，用户需要一种有效的方法来分析与性能相关的日志数据（“先分析”）。可以突出显示具有异常行为的学习者和镜头，例如导致信心大幅下降的学习者或覆盖率差的镜头（“显示重要”）。在了解学习者/镜头在最终预测中的作用后，他们可以决定添加/删除哪些以提高性能（“交互和反馈”）。基于更新的学习者（镜头），推荐合适的镜头（学习者）进行另一轮分析（“再次分析”）。这种具有人在环中的迭代分析过程非常符合视觉分析法则 [9]，并启发我们开发视觉分析工具 FSLDiagnotor，用于调整学习者和镜头的选择。

FSLDiagnotor 背后的关键是它能够有效地识别和消除由选定的基础学习器和镜头引起的性能瓶颈。给定一组学习器和一组带有少量镜头的样本，我们考虑两个问题：1）找到能够很好地预测样本集合的多样化和合作学习器的子集，以及 2）删除低质量的镜头并推荐必要的新镜头充分代表样本集合。通过研究这两个问题的内在特征，我们将它们表述为稀疏子集选择，并开发了两种选择算法来推荐合适的学习者和镜头。然而，推荐并不总是完美的，可能包含一个或几个低质量的学习者/镜头。例如，可以推荐错误预测某些具有高置信度的样本的学习器，因为它被误认为是那些样本的表现良好的学习器。如果没有人工参与，这种低质量的学习者/镜头很难被发现和纠正。为了促进此类任务，结合了矩阵可视化和散点图来解释推荐学习者的预测行为和上下文中镜头的覆盖范围。基于对学习者和镜头行为的理解，用户可以改进学习者的选择并增强镜头以获得更好的性能。